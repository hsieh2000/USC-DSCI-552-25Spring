{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Tien-Ching Hsieh  \n",
    "Github Username: hsieh2000  \n",
    "USC ID: 2642366337  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Anuran Calls (MFCCs)/Frogs_MFCCs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7, random_state=42, )\n",
    "df_test = df[~df.index.isin(df_train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_train.iloc[:, :-4]\n",
    "df_train_y = df_train.iloc[:, -4:-1]\n",
    "\n",
    "df_test_X = df_test.iloc[:, :-4]\n",
    "df_test_y = df_test.iloc[:, -4:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCCs_ 1</th>\n",
       "      <th>MFCCs_ 2</th>\n",
       "      <th>MFCCs_ 3</th>\n",
       "      <th>MFCCs_ 4</th>\n",
       "      <th>MFCCs_ 5</th>\n",
       "      <th>MFCCs_ 6</th>\n",
       "      <th>MFCCs_ 7</th>\n",
       "      <th>MFCCs_ 8</th>\n",
       "      <th>MFCCs_ 9</th>\n",
       "      <th>MFCCs_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MFCCs_13</th>\n",
       "      <th>MFCCs_14</th>\n",
       "      <th>MFCCs_15</th>\n",
       "      <th>MFCCs_16</th>\n",
       "      <th>MFCCs_17</th>\n",
       "      <th>MFCCs_18</th>\n",
       "      <th>MFCCs_19</th>\n",
       "      <th>MFCCs_20</th>\n",
       "      <th>MFCCs_21</th>\n",
       "      <th>MFCCs_22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.147506</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.433466</td>\n",
       "      <td>0.169581</td>\n",
       "      <td>0.053135</td>\n",
       "      <td>-0.152020</td>\n",
       "      <td>-0.102427</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.128628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341701</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>-0.290680</td>\n",
       "      <td>0.016310</td>\n",
       "      <td>0.285756</td>\n",
       "      <td>0.166246</td>\n",
       "      <td>-0.061206</td>\n",
       "      <td>-0.191878</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.255371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.674174</td>\n",
       "      <td>0.767520</td>\n",
       "      <td>0.392177</td>\n",
       "      <td>0.035669</td>\n",
       "      <td>0.181548</td>\n",
       "      <td>-0.013677</td>\n",
       "      <td>0.187262</td>\n",
       "      <td>0.144679</td>\n",
       "      <td>-0.424524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142909</td>\n",
       "      <td>-0.209932</td>\n",
       "      <td>0.285255</td>\n",
       "      <td>0.103722</td>\n",
       "      <td>-0.157943</td>\n",
       "      <td>-0.048913</td>\n",
       "      <td>-0.016129</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.011245</td>\n",
       "      <td>-0.275909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007777</td>\n",
       "      <td>0.363276</td>\n",
       "      <td>0.413224</td>\n",
       "      <td>0.196296</td>\n",
       "      <td>0.222389</td>\n",
       "      <td>0.097128</td>\n",
       "      <td>-0.155608</td>\n",
       "      <td>-0.035013</td>\n",
       "      <td>0.133689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>-0.028317</td>\n",
       "      <td>-0.102525</td>\n",
       "      <td>-0.005594</td>\n",
       "      <td>0.125169</td>\n",
       "      <td>0.044882</td>\n",
       "      <td>-0.013309</td>\n",
       "      <td>-0.026086</td>\n",
       "      <td>-0.088825</td>\n",
       "      <td>-0.018968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.429359</td>\n",
       "      <td>0.297881</td>\n",
       "      <td>0.609120</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>-0.030414</td>\n",
       "      <td>-0.160778</td>\n",
       "      <td>0.076217</td>\n",
       "      <td>0.285909</td>\n",
       "      <td>0.052029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374639</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>-0.317314</td>\n",
       "      <td>-0.023621</td>\n",
       "      <td>0.230330</td>\n",
       "      <td>0.069015</td>\n",
       "      <td>-0.101196</td>\n",
       "      <td>-0.152351</td>\n",
       "      <td>0.039065</td>\n",
       "      <td>0.233823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.190599</td>\n",
       "      <td>0.062234</td>\n",
       "      <td>0.617262</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>-0.214441</td>\n",
       "      <td>0.009654</td>\n",
       "      <td>0.299053</td>\n",
       "      <td>0.039013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402628</td>\n",
       "      <td>-0.110486</td>\n",
       "      <td>-0.306343</td>\n",
       "      <td>0.082182</td>\n",
       "      <td>0.321598</td>\n",
       "      <td>0.128742</td>\n",
       "      <td>-0.152977</td>\n",
       "      <td>-0.223482</td>\n",
       "      <td>0.016131</td>\n",
       "      <td>0.250754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.199469</td>\n",
       "      <td>0.560605</td>\n",
       "      <td>0.528894</td>\n",
       "      <td>0.261280</td>\n",
       "      <td>0.308324</td>\n",
       "      <td>0.078548</td>\n",
       "      <td>-0.111152</td>\n",
       "      <td>-0.052433</td>\n",
       "      <td>0.083926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050593</td>\n",
       "      <td>0.042922</td>\n",
       "      <td>-0.112277</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>-0.025855</td>\n",
       "      <td>0.031765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4470</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.143984</td>\n",
       "      <td>0.167472</td>\n",
       "      <td>0.574652</td>\n",
       "      <td>0.235745</td>\n",
       "      <td>0.083787</td>\n",
       "      <td>-0.091411</td>\n",
       "      <td>-0.012954</td>\n",
       "      <td>0.173614</td>\n",
       "      <td>-0.026243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231500</td>\n",
       "      <td>-0.278516</td>\n",
       "      <td>-0.226757</td>\n",
       "      <td>0.244491</td>\n",
       "      <td>0.259515</td>\n",
       "      <td>-0.110291</td>\n",
       "      <td>-0.226197</td>\n",
       "      <td>-0.103499</td>\n",
       "      <td>0.150326</td>\n",
       "      <td>0.214336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264793</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.401815</td>\n",
       "      <td>0.480250</td>\n",
       "      <td>0.234050</td>\n",
       "      <td>-0.197399</td>\n",
       "      <td>-0.249067</td>\n",
       "      <td>0.117561</td>\n",
       "      <td>0.242193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009119</td>\n",
       "      <td>0.181912</td>\n",
       "      <td>0.121719</td>\n",
       "      <td>-0.100493</td>\n",
       "      <td>-0.133644</td>\n",
       "      <td>0.041198</td>\n",
       "      <td>0.128915</td>\n",
       "      <td>-0.019947</td>\n",
       "      <td>-0.119334</td>\n",
       "      <td>-0.076849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.301851</td>\n",
       "      <td>0.251113</td>\n",
       "      <td>0.521005</td>\n",
       "      <td>0.207875</td>\n",
       "      <td>0.188435</td>\n",
       "      <td>-0.090052</td>\n",
       "      <td>-0.064887</td>\n",
       "      <td>0.169501</td>\n",
       "      <td>-0.050759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408540</td>\n",
       "      <td>-0.142526</td>\n",
       "      <td>-0.277716</td>\n",
       "      <td>0.178983</td>\n",
       "      <td>0.195275</td>\n",
       "      <td>-0.062219</td>\n",
       "      <td>-0.062185</td>\n",
       "      <td>-0.061390</td>\n",
       "      <td>0.076945</td>\n",
       "      <td>0.144903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206293</td>\n",
       "      <td>0.328579</td>\n",
       "      <td>0.328799</td>\n",
       "      <td>0.140071</td>\n",
       "      <td>0.197247</td>\n",
       "      <td>0.139127</td>\n",
       "      <td>-0.135449</td>\n",
       "      <td>-0.050155</td>\n",
       "      <td>0.097006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091243</td>\n",
       "      <td>-0.042775</td>\n",
       "      <td>-0.093499</td>\n",
       "      <td>0.053655</td>\n",
       "      <td>0.094706</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>-0.014292</td>\n",
       "      <td>-0.039948</td>\n",
       "      <td>-0.058637</td>\n",
       "      <td>0.020239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5036 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
       "3340       1.0  0.147506  0.014843  0.433466  0.169581  0.053135 -0.152020   \n",
       "5108       1.0  0.674174  0.767520  0.392177  0.035669  0.181548 -0.013677   \n",
       "6524       1.0 -0.007777  0.363276  0.413224  0.196296  0.222389  0.097128   \n",
       "3649       1.0  0.429359  0.297881  0.609120  0.263991 -0.030414 -0.160778   \n",
       "1617       1.0  0.190599  0.062234  0.617262  0.208825  0.027108 -0.214441   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5531       1.0 -0.199469  0.560605  0.528894  0.261280  0.308324  0.078548   \n",
       "4470       1.0  0.143984  0.167472  0.574652  0.235745  0.083787 -0.091411   \n",
       "891        1.0  0.264793  0.000276  0.401815  0.480250  0.234050 -0.197399   \n",
       "2793       1.0  0.301851  0.251113  0.521005  0.207875  0.188435 -0.090052   \n",
       "6041       1.0  0.206293  0.328579  0.328799  0.140071  0.197247  0.139127   \n",
       "\n",
       "      MFCCs_ 8  MFCCs_ 9  MFCCs_10  ...  MFCCs_13  MFCCs_14  MFCCs_15  \\\n",
       "3340 -0.102427  0.153061  0.128628  ...  0.341701  0.008463 -0.290680   \n",
       "5108  0.187262  0.144679 -0.424524  ... -0.142909 -0.209932  0.285255   \n",
       "6524 -0.155608 -0.035013  0.133689  ...  0.016052 -0.028317 -0.102525   \n",
       "3649  0.076217  0.285909  0.052029  ...  0.374639  0.012647 -0.317314   \n",
       "1617  0.009654  0.299053  0.039013  ...  0.402628 -0.110486 -0.306343   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5531 -0.111152 -0.052433  0.083926  ...  0.050593  0.042922 -0.112277   \n",
       "4470 -0.012954  0.173614 -0.026243  ...  0.231500 -0.278516 -0.226757   \n",
       "891  -0.249067  0.117561  0.242193  ... -0.009119  0.181912  0.121719   \n",
       "2793 -0.064887  0.169501 -0.050759  ...  0.408540 -0.142526 -0.277716   \n",
       "6041 -0.135449 -0.050155  0.097006  ...  0.091243 -0.042775 -0.093499   \n",
       "\n",
       "      MFCCs_16  MFCCs_17  MFCCs_18  MFCCs_19  MFCCs_20  MFCCs_21  MFCCs_22  \n",
       "3340  0.016310  0.285756  0.166246 -0.061206 -0.191878  0.003221  0.255371  \n",
       "5108  0.103722 -0.157943 -0.048913 -0.016129  0.129200  0.011245 -0.275909  \n",
       "6524 -0.005594  0.125169  0.044882 -0.013309 -0.026086 -0.088825 -0.018968  \n",
       "3649 -0.023621  0.230330  0.069015 -0.101196 -0.152351  0.039065  0.233823  \n",
       "1617  0.082182  0.321598  0.128742 -0.152977 -0.223482  0.016131  0.250754  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5531  0.004823  0.110200  0.006975  0.002524  0.005663 -0.025855  0.031765  \n",
       "4470  0.244491  0.259515 -0.110291 -0.226197 -0.103499  0.150326  0.214336  \n",
       "891  -0.100493 -0.133644  0.041198  0.128915 -0.019947 -0.119334 -0.076849  \n",
       "2793  0.178983  0.195275 -0.062219 -0.062185 -0.061390  0.076945  0.144903  \n",
       "6041  0.053655  0.094706  0.008469 -0.014292 -0.039948 -0.058637  0.020239  \n",
       "\n",
       "[5036 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(b)  \n",
    "### i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact match metric and Hamming loss are both for mult-label classification.\n",
    "\n",
    "Exact match metric: the function calculates subset accuracy meaning the predicted set of labels should exact match with the true set of label, which is also called accuracy score.  \n",
    "Hamming loss: The fraction of the wrong labels to the total number of label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, make_scorer, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a SVM for each of the labels => in single label classification hamming loss == 1 - accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gridcv_single_label(X_train, y_train, X_test, y_test, kernel, param_grid, scoring, k_fold = 10, refit = True):\n",
    "    model = SVC(kernel=kernel, decision_function_shape = 'ovr', random_state = 42)\n",
    "    gridcv = GridSearchCV(model, param_grid, scoring = scoring, cv = k_fold, refit = True)\n",
    "    gridcv.fit(X_train, y_train)\n",
    "    print(f'Kernel: {kernel}')\n",
    "    print(f'best model: {gridcv.best_estimator_}')\n",
    "\n",
    "    print(f'mean_test_score: {gridcv.cv_results_[\"mean_test_score\"]}')\n",
    "\n",
    "    y_predict = gridcv.predict(X_test)\n",
    "\n",
    "    print(f'accuracy score: {accuracy_score(y_test, y_predict)}')\n",
    "    print(f'hamming loss: {1 - accuracy_score(y_test, y_predict)}')\n",
    "\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    return gridcv\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [ 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    # 'kernel': [],\n",
    "}\n",
    "\n",
    "# scoring = {'accuracy score': make_scorer(accuracy_score), 'hamming loss': make_scorer(hamming_loss)}\n",
    "model = SVC(kernel='rbf', decision_function_shape = 'ovr', random_state = 42)\n",
    "k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Family\n",
      "Kernel: rbf\n",
      "best model: SVC(C=10, gamma=1, random_state=42)\n",
      "mean_test_score: [0.61498509 0.61538191 0.86060739 0.782574   0.61498509 0.61498509\n",
      " 0.61975173 0.85981137 0.95849703 0.97617154 0.71387232 0.61498509\n",
      " 0.85861932 0.93844552 0.98411397 0.9870945  0.79071957 0.62570687\n",
      " 0.92692851 0.96703588 0.99165996 0.98768973 0.80342627 0.62789099\n",
      " 0.94698121 0.98212826 0.9902691  0.98768973 0.80342627 0.62789099\n",
      " 0.97061007 0.98510721 0.9902691  0.98768973 0.80342627 0.62789099]\n",
      "accuracy score: 0.9911996294580825\n",
      "hamming loss: 0.008800370541917513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        15\n",
      "           1       0.99      1.00      1.00       163\n",
      "           2       0.99      0.98      0.99       658\n",
      "           3       0.99      0.99      0.99      1323\n",
      "\n",
      "    accuracy                           0.99      2159\n",
      "   macro avg       0.96      0.96      0.96      2159\n",
      "weighted avg       0.99      0.99      0.99      2159\n",
      "\n",
      "Label: Genus\n",
      "Kernel: rbf\n",
      "best model: SVC(C=100, gamma=1, random_state=42)\n",
      "mean_test_score: [0.57546349 0.57546349 0.78694026 0.72239855 0.57546349 0.57546349\n",
      " 0.57546349 0.80064376 0.93348717 0.91640396 0.67434678 0.57546349\n",
      " 0.80004773 0.93130187 0.98233022 0.97855841 0.74225204 0.58479559\n",
      " 0.92931656 0.97021679 0.9890802  0.97994691 0.75754205 0.58737693\n",
      " 0.9592966  0.98451434 0.98927901 0.97994691 0.75754205 0.58737693\n",
      " 0.97319575 0.98630163 0.98927901 0.97994691 0.75754205 0.58737693]\n",
      "accuracy score: 0.9893469198703103\n",
      "hamming loss: 0.010653080129689685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1252\n",
      "           1       0.99      1.00      1.00       163\n",
      "           2       0.97      0.96      0.96        92\n",
      "           3       0.99      0.99      0.99       479\n",
      "           4       0.96      0.97      0.97        71\n",
      "           5       0.97      0.85      0.91        34\n",
      "           6       0.93      0.93      0.93        15\n",
      "           7       1.00      0.96      0.98        53\n",
      "\n",
      "    accuracy                           0.99      2159\n",
      "   macro avg       0.98      0.96      0.97      2159\n",
      "weighted avg       0.99      0.99      0.99      2159\n",
      "\n",
      "Label: Species\n",
      "Kernel: rbf\n",
      "best model: SVC(C=10, gamma=1, random_state=42)\n",
      "mean_test_score: [0.4805378  0.48530247 0.83657618 0.65905085 0.4805378  0.4805378\n",
      " 0.51926102 0.84094441 0.93685112 0.92692693 0.57922662 0.4805378\n",
      " 0.84352575 0.93963323 0.98590086 0.97636956 0.67355707 0.48907547\n",
      " 0.93983125 0.97319338 0.99007148 0.97835487 0.69500339 0.49185601\n",
      " 0.96902198 0.98530602 0.98947624 0.97835487 0.69500339 0.49185601\n",
      " 0.98212984 0.98590323 0.98947624 0.97835487 0.69500339 0.49185601]\n",
      "accuracy score: 0.9893469198703103\n",
      "hamming loss: 0.010653080129689685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       194\n",
      "           1       1.00      1.00      1.00      1058\n",
      "           2       1.00      1.00      1.00       163\n",
      "           3       0.96      0.97      0.96        92\n",
      "           4       0.98      0.98      0.98       145\n",
      "           5       0.99      0.99      0.99       334\n",
      "           6       0.96      0.97      0.97        71\n",
      "           7       1.00      0.91      0.95        34\n",
      "           8       0.93      0.87      0.90        15\n",
      "           9       1.00      0.96      0.98        53\n",
      "\n",
      "    accuracy                           0.99      2159\n",
      "   macro avg       0.98      0.96      0.97      2159\n",
      "weighted avg       0.99      0.99      0.99      2159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label in ['Family', 'Genus', 'Species']:\n",
    "    train_X = df_train_X.copy()\n",
    "    test_X = df_test_X.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_y = label_encoder.fit_transform(df_train_y[label])\n",
    "    test_y = label_encoder.transform(df_test_y[label])\n",
    "\n",
    "    print(f'Label: {label}')\n",
    "    result = run_gridcv_single_label(train_X, \n",
    "        train_y, \n",
    "        test_X, \n",
    "        test_y, \n",
    "        'rbf', \n",
    "        param_grid, \n",
    "        'accuracy', \n",
    "        k_fold = k_fold, \n",
    "        refit = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data\n",
      "Label: Family\n",
      "Kernel: rbf\n",
      "best model: SVC(C=100, gamma=0.01, random_state=42)\n",
      "mean_test_score: [0.83577898 0.84611103 0.63880487 0.61497483 0.61497483 0.61497483\n",
      " 0.94459434 0.97418584 0.74524007 0.61835219 0.61497483 0.61497483\n",
      " 0.96683826 0.98888021 0.90210523 0.65826667 0.61636569 0.61497483\n",
      " 0.98590284 0.98967465 0.90925203 0.66919081 0.61716053 0.61497483\n",
      " 0.98987504 0.98967465 0.90925203 0.66919081 0.61716053 0.61497483\n",
      " 0.98888139 0.98967465 0.90925203 0.66919081 0.61716053 0.61497483]\n",
      "accuracy score: 0.9911996294580825\n",
      "hamming loss: 0.008800370541917513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.99      1.00      0.99       163\n",
      "           2       0.99      0.98      0.99       658\n",
      "           3       0.99      1.00      0.99      1323\n",
      "\n",
      "    accuracy                           0.99      2159\n",
      "   macro avg       0.96      0.98      0.97      2159\n",
      "weighted avg       0.99      0.99      0.99      2159\n",
      "\n",
      "Label: Genus\n",
      "Kernel: rbf\n",
      "best model: SVC(C=10, gamma=0.01, random_state=42)\n",
      "mean_test_score: [0.78137169 0.77521301 0.59709639 0.57545639 0.57545639 0.57545639\n",
      " 0.90766039 0.94876771 0.7027403  0.57922939 0.57545639 0.57545639\n",
      " 0.96942117 0.98451355 0.85225749 0.61536732 0.57684646 0.57545639\n",
      " 0.98609967 0.98590323 0.86297887 0.62192007 0.57744209 0.57545639\n",
      " 0.98570403 0.98590323 0.86297887 0.62192007 0.57744209 0.57545639\n",
      " 0.98590284 0.98590323 0.86297887 0.62192007 0.57744209 0.57545639]\n",
      "accuracy score: 0.9888837424733673\n",
      "hamming loss: 0.0111162575266327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1252\n",
      "           1       0.99      1.00      0.99       163\n",
      "           2       0.96      0.96      0.96        92\n",
      "           3       0.99      0.99      0.99       479\n",
      "           4       0.97      0.97      0.97        71\n",
      "           5       0.97      0.88      0.92        34\n",
      "           6       0.93      0.87      0.90        15\n",
      "           7       1.00      0.98      0.99        53\n",
      "\n",
      "    accuracy                           0.99      2159\n",
      "   macro avg       0.97      0.96      0.96      2159\n",
      "weighted avg       0.99      0.99      0.99      2159\n",
      "\n",
      "Label: Species\n",
      "Kernel: rbf\n",
      "best model: SVC(C=100, gamma=0.01, random_state=42)\n",
      "mean_test_score: [0.73431475 0.81393583 0.50178335 0.48053544 0.48053544 0.48053544\n",
      " 0.91699722 0.95412998 0.62390104 0.48410963 0.48053544 0.48053544\n",
      " 0.97180686 0.98113502 0.82645121 0.51826225 0.4815275  0.48053544\n",
      " 0.9866957  0.98312151 0.83995156 0.5248146  0.48232194 0.48053544\n",
      " 0.98887982 0.98312151 0.83995156 0.5248146  0.48232194 0.48053544\n",
      " 0.98848299 0.98312151 0.83995156 0.5248146  0.48232194 0.48053544]\n",
      "accuracy score: 0.9884205650764243\n",
      "hamming loss: 0.011579434923575715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       194\n",
      "           1       1.00      1.00      1.00      1058\n",
      "           2       1.00      0.99      1.00       163\n",
      "           3       0.96      0.99      0.97        92\n",
      "           4       0.97      0.97      0.97       145\n",
      "           5       0.99      0.98      0.98       334\n",
      "           6       1.00      0.97      0.99        71\n",
      "           7       0.91      0.85      0.88        34\n",
      "           8       1.00      1.00      1.00        15\n",
      "           9       1.00      0.98      0.99        53\n",
      "\n",
      "    accuracy                           0.99      2159\n",
      "   macro avg       0.98      0.97      0.98      2159\n",
      "weighted avg       0.99      0.99      0.99      2159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"standardized data\")\n",
    "for label in ['Family', 'Genus', 'Species']:\n",
    "    train_X = df_train_X.copy()\n",
    "    test_X = df_test_X.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_y = label_encoder.fit_transform(df_train_y[label])\n",
    "    test_y = label_encoder.transform(df_test_y[label])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "\n",
    "    print(f'Label: {label}')\n",
    "    result = run_gridcv_single_label(\n",
    "        train_X, \n",
    "        train_y, \n",
    "        test_X, \n",
    "        test_y, \n",
    "        'rbf', \n",
    "        param_grid, \n",
    "        'accuracy', \n",
    "        k_fold = k_fold, \n",
    "        refit = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [ 0.01, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "# scoring = {'accuracy score': make_scorer(accuracy_score), 'hamming loss': make_scorer(hamming_loss)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Family\n",
      "Kernel: linear\n",
      "best model: SVC(C=100, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.82088974 0.92018011 0.93784949 0.94579152 0.94758    0.94519865]\n",
      "accuracy score: 0.9564613246873552\n",
      "hamming loss: 0.04353867531264477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.87      0.60        15\n",
      "           1       0.92      0.95      0.93       163\n",
      "           2       0.96      0.92      0.94       658\n",
      "           3       0.97      0.98      0.97      1323\n",
      "\n",
      "    accuracy                           0.96      2159\n",
      "   macro avg       0.83      0.93      0.86      2159\n",
      "weighted avg       0.96      0.96      0.96      2159\n",
      "\n",
      "Label: Genus\n",
      "Kernel: linear\n",
      "best model: SVC(C=100, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.71682603 0.91123655 0.94996055 0.96544937 0.96544976 0.9640589 ]\n",
      "accuracy score: 0.9731357109773043\n",
      "hamming loss: 0.026864289022695664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1252\n",
      "           1       0.96      0.94      0.95       163\n",
      "           2       0.91      0.82      0.86        92\n",
      "           3       0.98      0.98      0.98       479\n",
      "           4       1.00      0.97      0.99        71\n",
      "           5       0.94      0.88      0.91        34\n",
      "           6       0.87      0.87      0.87        15\n",
      "           7       0.96      0.98      0.97        53\n",
      "\n",
      "    accuracy                           0.97      2159\n",
      "   macro avg       0.95      0.93      0.94      2159\n",
      "weighted avg       0.97      0.97      0.97      2159\n",
      "\n",
      "Label: Species\n",
      "Kernel: linear\n",
      "best model: SVC(C=10, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.80242908 0.92057615 0.96445809 0.97915286 0.97815725 0.97418584]\n",
      "accuracy score: 0.9791570171375636\n",
      "hamming loss: 0.020842982862436354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       194\n",
      "           1       1.00      1.00      1.00      1058\n",
      "           2       0.97      0.96      0.96       163\n",
      "           3       0.90      0.88      0.89        92\n",
      "           4       0.97      0.97      0.97       145\n",
      "           5       0.99      0.99      0.99       334\n",
      "           6       0.99      0.96      0.97        71\n",
      "           7       0.91      0.85      0.88        34\n",
      "           8       0.93      0.93      0.93        15\n",
      "           9       1.00      0.94      0.97        53\n",
      "\n",
      "    accuracy                           0.98      2159\n",
      "   macro avg       0.96      0.95      0.95      2159\n",
      "weighted avg       0.98      0.98      0.98      2159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label in ['Family', 'Genus', 'Species']:\n",
    "    train_X = df_train_X.copy()\n",
    "    test_X = df_test_X.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_y = label_encoder.fit_transform(df_train_y[label])\n",
    "    test_y = label_encoder.transform(df_test_y[label])\n",
    "\n",
    "    print(f'Label: {label}')\n",
    "    result = run_gridcv_single_label(train_X, \n",
    "        train_y, \n",
    "        test_X, \n",
    "        test_y, \n",
    "        'linear', \n",
    "        param_grid, \n",
    "        'accuracy', \n",
    "        k_fold = k_fold, \n",
    "        refit = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data\n",
      "Label: Family\n",
      "Kernel: linear\n",
      "best model: SVC(C=1, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.9392439  0.94242127 0.94877402 0.94817681 0.94738316 0.94797879]\n",
      "accuracy score: 0.9564613246873552\n",
      "hamming loss: 0.04353867531264477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.87      0.62        15\n",
      "           1       0.92      0.94      0.93       163\n",
      "           2       0.96      0.92      0.94       658\n",
      "           3       0.97      0.98      0.97      1323\n",
      "\n",
      "    accuracy                           0.96      2159\n",
      "   macro avg       0.83      0.93      0.87      2159\n",
      "weighted avg       0.96      0.96      0.96      2159\n",
      "\n",
      "Label: Genus\n",
      "Kernel: linear\n",
      "best model: SVC(C=0.1, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.9428102  0.96544937 0.96525174 0.96306526 0.96187557 0.96147835]\n",
      "accuracy score: 0.9712830013895322\n",
      "hamming loss: 0.028716998610467837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1252\n",
      "           1       0.95      0.96      0.95       163\n",
      "           2       0.88      0.72      0.79        92\n",
      "           3       0.97      0.99      0.98       479\n",
      "           4       0.99      0.97      0.98        71\n",
      "           5       0.96      0.79      0.87        34\n",
      "           6       1.00      0.87      0.93        15\n",
      "           7       0.98      0.96      0.97        53\n",
      "\n",
      "    accuracy                           0.97      2159\n",
      "   macro avg       0.96      0.91      0.93      2159\n",
      "weighted avg       0.97      0.97      0.97      2159\n",
      "\n",
      "Label: Species\n",
      "Kernel: linear\n",
      "best model: SVC(C=1, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.95730695 0.97418505 0.97954653 0.97676639 0.97378783 0.97418544]\n",
      "accuracy score: 0.9828624363131079\n",
      "hamming loss: 0.01713756368689212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       194\n",
      "           1       1.00      1.00      1.00      1058\n",
      "           2       0.97      0.95      0.96       163\n",
      "           3       0.92      0.93      0.93        92\n",
      "           4       0.98      0.98      0.98       145\n",
      "           5       0.98      0.99      0.99       334\n",
      "           6       0.99      0.97      0.98        71\n",
      "           7       0.97      0.91      0.94        34\n",
      "           8       0.87      0.87      0.87        15\n",
      "           9       1.00      0.96      0.98        53\n",
      "\n",
      "    accuracy                           0.98      2159\n",
      "   macro avg       0.96      0.95      0.96      2159\n",
      "weighted avg       0.98      0.98      0.98      2159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"standardized data\")\n",
    "for label in ['Family', 'Genus', 'Species']:\n",
    "    train_X = df_train_X.copy()\n",
    "    test_X = df_test_X.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_y = label_encoder.fit_transform(df_train_y[label])\n",
    "    test_y = label_encoder.transform(df_test_y[label])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "\n",
    "    print(f'Label: {label}')\n",
    "    result = run_gridcv_single_label(train_X, \n",
    "        train_y, \n",
    "        test_X, \n",
    "        test_y, \n",
    "        'linear', \n",
    "        param_grid, \n",
    "        'accuracy', \n",
    "        k_fold = k_fold, \n",
    "        refit = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [ 0.01, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "k_fold = KFold(n_splits=10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE: Counter({np.int64(3): 3097, np.int64(2): 3097, np.int64(1): 3097, np.int64(0): 3097})\n",
      "Label: Family\n",
      "Kernel: linear\n",
      "best model: SVC(C=1000, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.89029723 0.92904448 0.95431096 0.96359361 0.96593434 0.96641886]\n",
      "accuracy score: 0.9495136637332098\n",
      "hamming loss: 0.05048633626679022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      1.00      0.55        15\n",
      "           1       0.85      0.98      0.91       163\n",
      "           2       0.95      0.93      0.94       658\n",
      "           3       0.98      0.95      0.97      1323\n",
      "\n",
      "    accuracy                           0.95      2159\n",
      "   macro avg       0.79      0.97      0.84      2159\n",
      "weighted avg       0.96      0.95      0.95      2159\n",
      "\n",
      "SMOTE: Counter({np.int64(0): 2898, np.int64(3): 2898, np.int64(1): 2898, np.int64(2): 2898, np.int64(7): 2898, np.int64(4): 2898, np.int64(6): 2898, np.int64(5): 2898})\n",
      "Label: Genus\n",
      "Kernel: linear\n",
      "best model: SVC(C=1000, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.87620761 0.96618364 0.98343688 0.98956186 0.99128723 0.99141667]\n",
      "accuracy score: 0.9722093561834182\n",
      "hamming loss: 0.027790643816581806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1252\n",
      "           1       0.94      0.97      0.95       163\n",
      "           2       0.84      0.95      0.89        92\n",
      "           3       0.98      0.97      0.97       479\n",
      "           4       0.97      0.97      0.97        71\n",
      "           5       0.83      0.85      0.84        34\n",
      "           6       0.88      0.93      0.90        15\n",
      "           7       0.96      0.94      0.95        53\n",
      "\n",
      "    accuracy                           0.97      2159\n",
      "   macro avg       0.92      0.95      0.93      2159\n",
      "weighted avg       0.97      0.97      0.97      2159\n",
      "\n",
      "SMOTE: Counter({np.int64(1): 2420, np.int64(4): 2420, np.int64(5): 2420, np.int64(0): 2420, np.int64(2): 2420, np.int64(3): 2420, np.int64(9): 2420, np.int64(6): 2420, np.int64(8): 2420, np.int64(7): 2420})\n",
      "Label: Species\n",
      "Kernel: linear\n",
      "best model: SVC(C=1000, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.89553719 0.95971074 0.98442149 0.99227273 0.99487603 0.99504132]\n",
      "accuracy score: 0.9791570171375636\n",
      "hamming loss: 0.020842982862436354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       194\n",
      "           1       0.99      1.00      1.00      1058\n",
      "           2       0.97      0.94      0.96       163\n",
      "           3       0.91      0.89      0.90        92\n",
      "           4       0.95      0.99      0.97       145\n",
      "           5       0.99      0.98      0.98       334\n",
      "           6       0.96      0.99      0.97        71\n",
      "           7       1.00      0.82      0.90        34\n",
      "           8       0.93      0.93      0.93        15\n",
      "           9       1.00      0.94      0.97        53\n",
      "\n",
      "    accuracy                           0.98      2159\n",
      "   macro avg       0.97      0.95      0.95      2159\n",
      "weighted avg       0.98      0.98      0.98      2159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for label in ['Family', 'Genus', 'Species']:\n",
    "    \n",
    "    train_X = df_train_X.copy()\n",
    "    test_X = df_test_X.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_y = label_encoder.fit_transform(df_train_y[label])\n",
    "    test_y = label_encoder.transform(df_test_y[label])\n",
    "\n",
    "    smote = SMOTE()\n",
    "    train_X, train_y = smote.fit_resample(train_X, train_y)\n",
    "    print(f'SMOTE: {Counter(train_y)}')\n",
    "    \n",
    "    print(f'Label: {label}')\n",
    "    result = run_gridcv_single_label(\n",
    "        train_X, \n",
    "        train_y, \n",
    "        test_X, \n",
    "        test_y, \n",
    "        'linear', \n",
    "        param_grid, \n",
    "        'accuracy', \n",
    "        k_fold = k_fold, \n",
    "        refit = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [ 0.01, 1, 100, 1000],\n",
    "}\n",
    "k_fold = KFold(n_splits=10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data\n",
      "SMOTE: Counter({np.int64(3): 3097, np.int64(2): 3097, np.int64(1): 3097, np.int64(0): 3097})\n",
      "Label: Family\n",
      "Kernel: linear\n",
      "best model: SVC(C=1000, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.95414999 0.9658548  0.96690397 0.96714604]\n",
      "accuracy score: 0.952755905511811\n",
      "hamming loss: 0.047244094488189003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      1.00      0.59        15\n",
      "           1       0.86      0.98      0.92       163\n",
      "           2       0.95      0.94      0.94       658\n",
      "           3       0.98      0.96      0.97      1323\n",
      "\n",
      "    accuracy                           0.95      2159\n",
      "   macro avg       0.80      0.97      0.85      2159\n",
      "weighted avg       0.96      0.95      0.95      2159\n",
      "\n",
      "SMOTE: Counter({np.int64(0): 2898, np.int64(3): 2898, np.int64(1): 2898, np.int64(2): 2898, np.int64(7): 2898, np.int64(4): 2898, np.int64(6): 2898, np.int64(5): 2898})\n",
      "Label: Genus\n",
      "Kernel: linear\n",
      "best model: SVC(C=100, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.98240174 0.99154583 0.99189082 0.9915026 ]\n",
      "accuracy score: 0.9708198239925891\n",
      "hamming loss: 0.029180176007410852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1252\n",
      "           1       0.96      0.94      0.95       163\n",
      "           2       0.78      0.95      0.86        92\n",
      "           3       0.97      0.97      0.97       479\n",
      "           4       0.95      0.97      0.96        71\n",
      "           5       0.80      0.82      0.81        34\n",
      "           6       1.00      0.93      0.97        15\n",
      "           7       1.00      0.94      0.97        53\n",
      "\n",
      "    accuracy                           0.97      2159\n",
      "   macro avg       0.93      0.94      0.93      2159\n",
      "weighted avg       0.97      0.97      0.97      2159\n",
      "\n",
      "SMOTE: Counter({np.int64(1): 2420, np.int64(4): 2420, np.int64(5): 2420, np.int64(0): 2420, np.int64(2): 2420, np.int64(3): 2420, np.int64(9): 2420, np.int64(6): 2420, np.int64(8): 2420, np.int64(7): 2420})\n",
      "Label: Species\n",
      "Kernel: linear\n",
      "best model: SVC(C=1000, kernel='linear', random_state=42)\n",
      "mean_test_score: [0.98144628 0.99549587 0.99694215 0.99698347]\n",
      "accuracy score: 0.9777674849467346\n",
      "hamming loss: 0.0222325150532654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       194\n",
      "           1       1.00      1.00      1.00      1058\n",
      "           2       0.97      0.94      0.96       163\n",
      "           3       0.92      0.90      0.91        92\n",
      "           4       0.92      0.98      0.95       145\n",
      "           5       0.99      0.98      0.98       334\n",
      "           6       0.96      0.97      0.97        71\n",
      "           7       0.96      0.79      0.87        34\n",
      "           8       1.00      0.93      0.97        15\n",
      "           9       1.00      0.96      0.98        53\n",
      "\n",
      "    accuracy                           0.98      2159\n",
      "   macro avg       0.97      0.94      0.95      2159\n",
      "weighted avg       0.98      0.98      0.98      2159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"standardized data\")\n",
    "for label in ['Family', 'Genus', 'Species']:\n",
    "    \n",
    "    train_X = df_train_X.copy()\n",
    "    test_X = df_test_X.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_y = label_encoder.fit_transform(df_train_y[label])\n",
    "    test_y = label_encoder.transform(df_test_y[label])\n",
    "\n",
    "    smote = SMOTE()\n",
    "    train_X, train_y = smote.fit_resample(train_X, train_y)\n",
    "    print(f'SMOTE: {Counter(train_y)}')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "    \n",
    "    print(f'Label: {label}')\n",
    "    result = run_gridcv_single_label(\n",
    "        train_X, \n",
    "        train_y, \n",
    "        test_X, \n",
    "        test_y, \n",
    "        'linear', \n",
    "        param_grid, \n",
    "        'accuracy', \n",
    "        k_fold = k_fold, \n",
    "        refit = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Monte-Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCCs_ 1</th>\n",
       "      <th>MFCCs_ 2</th>\n",
       "      <th>MFCCs_ 3</th>\n",
       "      <th>MFCCs_ 4</th>\n",
       "      <th>MFCCs_ 5</th>\n",
       "      <th>MFCCs_ 6</th>\n",
       "      <th>MFCCs_ 7</th>\n",
       "      <th>MFCCs_ 8</th>\n",
       "      <th>MFCCs_ 9</th>\n",
       "      <th>MFCCs_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MFCCs_13</th>\n",
       "      <th>MFCCs_14</th>\n",
       "      <th>MFCCs_15</th>\n",
       "      <th>MFCCs_16</th>\n",
       "      <th>MFCCs_17</th>\n",
       "      <th>MFCCs_18</th>\n",
       "      <th>MFCCs_19</th>\n",
       "      <th>MFCCs_20</th>\n",
       "      <th>MFCCs_21</th>\n",
       "      <th>MFCCs_22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152936</td>\n",
       "      <td>-0.105586</td>\n",
       "      <td>0.200722</td>\n",
       "      <td>0.317201</td>\n",
       "      <td>0.260764</td>\n",
       "      <td>0.100945</td>\n",
       "      <td>-0.150063</td>\n",
       "      <td>-0.171128</td>\n",
       "      <td>0.124676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156436</td>\n",
       "      <td>0.082245</td>\n",
       "      <td>0.135752</td>\n",
       "      <td>-0.024017</td>\n",
       "      <td>-0.108351</td>\n",
       "      <td>-0.077623</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.057684</td>\n",
       "      <td>0.118680</td>\n",
       "      <td>0.014038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171534</td>\n",
       "      <td>-0.098975</td>\n",
       "      <td>0.268425</td>\n",
       "      <td>0.338672</td>\n",
       "      <td>0.268353</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>-0.222475</td>\n",
       "      <td>-0.207693</td>\n",
       "      <td>0.170883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254341</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.163320</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>-0.090974</td>\n",
       "      <td>-0.056510</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.029056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.082973</td>\n",
       "      <td>0.287128</td>\n",
       "      <td>0.276014</td>\n",
       "      <td>0.189867</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>-0.242234</td>\n",
       "      <td>-0.219153</td>\n",
       "      <td>0.232538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237384</td>\n",
       "      <td>0.050791</td>\n",
       "      <td>0.207338</td>\n",
       "      <td>0.083536</td>\n",
       "      <td>-0.050691</td>\n",
       "      <td>-0.023590</td>\n",
       "      <td>-0.066722</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>0.077162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224392</td>\n",
       "      <td>0.118985</td>\n",
       "      <td>0.329432</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.361005</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>-0.194347</td>\n",
       "      <td>-0.098181</td>\n",
       "      <td>0.270375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317084</td>\n",
       "      <td>-0.011567</td>\n",
       "      <td>0.100413</td>\n",
       "      <td>-0.050224</td>\n",
       "      <td>-0.136009</td>\n",
       "      <td>-0.177037</td>\n",
       "      <td>-0.130498</td>\n",
       "      <td>-0.054766</td>\n",
       "      <td>-0.018691</td>\n",
       "      <td>0.023954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087817</td>\n",
       "      <td>-0.068345</td>\n",
       "      <td>0.306967</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.249144</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>-0.265423</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>0.266434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298524</td>\n",
       "      <td>0.037439</td>\n",
       "      <td>0.219153</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>-0.048885</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>-0.088550</td>\n",
       "      <td>-0.031346</td>\n",
       "      <td>0.108610</td>\n",
       "      <td>0.079244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.554504</td>\n",
       "      <td>-0.337717</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.443451</td>\n",
       "      <td>0.093889</td>\n",
       "      <td>-0.100753</td>\n",
       "      <td>0.037087</td>\n",
       "      <td>0.081075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145668</td>\n",
       "      <td>-0.059364</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.071001</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.052449</td>\n",
       "      <td>-0.021860</td>\n",
       "      <td>-0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.517273</td>\n",
       "      <td>-0.370574</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.402890</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>-0.116460</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.089034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164675</td>\n",
       "      <td>-0.105600</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.061127</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.046461</td>\n",
       "      <td>-0.015418</td>\n",
       "      <td>-0.101892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.582557</td>\n",
       "      <td>-0.343237</td>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.385596</td>\n",
       "      <td>0.114905</td>\n",
       "      <td>-0.103317</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.081317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150025</td>\n",
       "      <td>-0.078615</td>\n",
       "      <td>0.024861</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.077771</td>\n",
       "      <td>-0.009688</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>-0.080425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.519497</td>\n",
       "      <td>-0.307553</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>0.377131</td>\n",
       "      <td>0.086866</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>0.089316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153120</td>\n",
       "      <td>-0.075320</td>\n",
       "      <td>0.022903</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.051796</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.041803</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>-0.096895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.508833</td>\n",
       "      <td>-0.324106</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>0.078211</td>\n",
       "      <td>0.397188</td>\n",
       "      <td>0.094596</td>\n",
       "      <td>-0.117672</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>0.076180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150554</td>\n",
       "      <td>-0.073415</td>\n",
       "      <td>0.042517</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.061455</td>\n",
       "      <td>0.072983</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.029355</td>\n",
       "      <td>-0.087910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7195 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
       "0          1.0  0.152936 -0.105586  0.200722  0.317201  0.260764  0.100945   \n",
       "1          1.0  0.171534 -0.098975  0.268425  0.338672  0.268353  0.060835   \n",
       "2          1.0  0.152317 -0.082973  0.287128  0.276014  0.189867  0.008714   \n",
       "3          1.0  0.224392  0.118985  0.329432  0.372088  0.361005  0.015501   \n",
       "4          1.0  0.087817 -0.068345  0.306967  0.330923  0.249144  0.006884   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7190       1.0 -0.554504 -0.337717  0.035533  0.034511  0.443451  0.093889   \n",
       "7191       1.0 -0.517273 -0.370574  0.030673  0.068097  0.402890  0.096628   \n",
       "7192       1.0 -0.582557 -0.343237  0.029468  0.064179  0.385596  0.114905   \n",
       "7193       1.0 -0.519497 -0.307553 -0.004922  0.072865  0.377131  0.086866   \n",
       "7194       1.0 -0.508833 -0.324106  0.062068  0.078211  0.397188  0.094596   \n",
       "\n",
       "      MFCCs_ 8  MFCCs_ 9  MFCCs_10  ...  MFCCs_13  MFCCs_14  MFCCs_15  \\\n",
       "0    -0.150063 -0.171128  0.124676  ... -0.156436  0.082245  0.135752   \n",
       "1    -0.222475 -0.207693  0.170883  ... -0.254341  0.022786  0.163320   \n",
       "2    -0.242234 -0.219153  0.232538  ... -0.237384  0.050791  0.207338   \n",
       "3    -0.194347 -0.098181  0.270375  ... -0.317084 -0.011567  0.100413   \n",
       "4    -0.265423 -0.172700  0.266434  ... -0.298524  0.037439  0.219153   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7190 -0.100753  0.037087  0.081075  ... -0.145668 -0.059364  0.024206   \n",
       "7191 -0.116460  0.063727  0.089034  ... -0.164675 -0.105600  0.030767   \n",
       "7192 -0.103317  0.070370  0.081317  ... -0.150025 -0.078615  0.024861   \n",
       "7193 -0.115799  0.056979  0.089316  ... -0.153120 -0.075320  0.022903   \n",
       "7194 -0.117672  0.058874  0.076180  ... -0.150554 -0.073415  0.042517   \n",
       "\n",
       "      MFCCs_16  MFCCs_17  MFCCs_18  MFCCs_19  MFCCs_20  MFCCs_21  MFCCs_22  \n",
       "0    -0.024017 -0.108351 -0.077623 -0.009568  0.057684  0.118680  0.014038  \n",
       "1     0.012022 -0.090974 -0.056510 -0.035303  0.020140  0.082263  0.029056  \n",
       "2     0.083536 -0.050691 -0.023590 -0.066722 -0.025083  0.099108  0.077162  \n",
       "3    -0.050224 -0.136009 -0.177037 -0.130498 -0.054766 -0.018691  0.023954  \n",
       "4     0.062837 -0.048885 -0.053074 -0.088550 -0.031346  0.108610  0.079244  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7190 -0.000861  0.069430  0.071001  0.021591  0.052449 -0.021860 -0.079860  \n",
       "7191  0.006457  0.061127  0.068978  0.017745  0.046461 -0.015418 -0.101892  \n",
       "7192  0.008696  0.082474  0.077771 -0.009688  0.027834 -0.000531 -0.080425  \n",
       "7193  0.001924  0.051796  0.069073  0.017963  0.041803 -0.027911 -0.096895  \n",
       "7194  0.004158  0.061455  0.072983 -0.003980  0.031560 -0.029355 -0.087910  \n",
       "\n",
       "[7195 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, :-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_k(K_range, X):\n",
    "    best_k = min(K_range)\n",
    "    max_score = 0\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        clusters = kmeans.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, clusters)\n",
    "        if silhouette_avg > max_score:\n",
    "            best_k = k\n",
    "            max_score = silhouette_avg\n",
    "\n",
    "    print(f'the best K = {best_k}')\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_label(best_k, clusters, Y):\n",
    "    cluster_major = pd.DataFrame(columns=Y.columns)\n",
    "    for c in range(best_k):\n",
    "        index_, = np.where(clusters == c)\n",
    "        cluster_samples = Y.iloc[index_, :]\n",
    "        row = []\n",
    "        for label in Y.columns:\n",
    "            major = cluster_samples.loc[:, label].value_counts().index[0]\n",
    "            row.append(major)\n",
    "        cluster_major.loc[c] = row\n",
    "    return cluster_major\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hamming_loss_dist(cluster_major, cluster_labels, Y):\n",
    "    missing_labels = 0\n",
    "    for c in range(len(cluster_major)):\n",
    "        index_, = np.where(cluster_labels == c)\n",
    "        for label in Y.loc[index_].values:\n",
    "            miss = (label != cluster_major.loc[c].values)\n",
    "            missing_labels += np.sum(miss)\n",
    "\n",
    "    hamming_dist = missing_labels/Y.shape[0]\n",
    "    hamming_loss = missing_labels/(Y.shape[0]*Y.shape[1])\n",
    "    \n",
    "    return hamming_dist, hamming_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "the best K = 3\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.8903405142460041\n",
      "hamming loss: 0.29678017141533475\n",
      "\n",
      "iteration: 2\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.7011813759555247\n",
      "hamming loss: 0.23372712531850823\n",
      "\n",
      "iteration: 3\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "4          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "5  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.5045170257123002\n",
      "hamming loss: 0.16817234190410008\n",
      "\n",
      "iteration: 4\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.8401667824878388\n",
      "hamming loss: 0.28005559416261294\n",
      "\n",
      "iteration: 5\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.6472550382209868\n",
      "hamming loss: 0.2157516794069956\n",
      "\n",
      "iteration: 6\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "4          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "5  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.5028492008339125\n",
      "hamming loss: 0.1676164002779708\n",
      "\n",
      "iteration: 7\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.6472550382209868\n",
      "hamming loss: 0.2157516794069956\n",
      "\n",
      "iteration: 8\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.7009034051424601\n",
      "hamming loss: 0.23363446838082003\n",
      "\n",
      "iteration: 9\n",
      "the best K = 6\n",
      "            Family          Genus                 Species\n",
      "0          Hylidae      Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae      Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae      Hypsiboas    HypsiboasCinerascens\n",
      "3  Leptodactylidae      Adenomera          AdenomeraAndre\n",
      "4          Hylidae      Hypsiboas       HypsiboasCordobae\n",
      "5  Leptodactylidae  Leptodactylus     LeptodactylusFuscus\n",
      "hamming distance: 0.6600416956219597\n",
      "hamming loss: 0.22001389854065323\n",
      "\n",
      "iteration: 10\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "4  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "5    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.5047949965253649\n",
      "hamming loss: 0.16826499884178828\n",
      "\n",
      "iteration: 11\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.7021542738012508\n",
      "hamming loss: 0.23405142460041695\n",
      "\n",
      "iteration: 12\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.7002084781097985\n",
      "hamming loss: 0.23340282603659948\n",
      "\n",
      "iteration: 13\n",
      "the best K = 3\n",
      "            Family      Genus                 Species\n",
      "0    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.7154968728283531\n",
      "hamming loss: 0.238498957609451\n",
      "\n",
      "iteration: 14\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.7357887421820709\n",
      "hamming loss: 0.24526291406069028\n",
      "\n",
      "iteration: 15\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.66726893676164\n",
      "hamming loss: 0.2224229789205467\n",
      "\n",
      "iteration: 16\n",
      "the best K = 3\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "hamming distance: 0.8472550382209868\n",
      "hamming loss: 0.2824183460736623\n",
      "\n",
      "iteration: 17\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "2    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.6674079221681724\n",
      "hamming loss: 0.22246930738939077\n",
      "\n",
      "iteration: 18\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.5581653926337734\n",
      "hamming loss: 0.1860551308779245\n",
      "\n",
      "iteration: 19\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.5581653926337734\n",
      "hamming loss: 0.1860551308779245\n",
      "\n",
      "iteration: 20\n",
      "the best K = 5\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "4    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.5032661570535094\n",
      "hamming loss: 0.16775538568450313\n",
      "\n",
      "iteration: 21\n",
      "the best K = 6\n",
      "            Family          Genus                 Species\n",
      "0  Leptodactylidae      Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae      Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae      Adenomera          AdenomeraAndre\n",
      "3  Leptodactylidae      Adenomera          AdenomeraAndre\n",
      "4          Hylidae      Hypsiboas    HypsiboasCinerascens\n",
      "5          Hylidae  Leptodactylus     LeptodactylusFuscus\n",
      "hamming distance: 0.5621959694232106\n",
      "hamming loss: 0.1873986564744035\n",
      "\n",
      "iteration: 22\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "hamming distance: 0.8560111188325226\n",
      "hamming loss: 0.28533703961084084\n",
      "\n",
      "iteration: 23\n",
      "the best K = 7\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "4  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "5  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "6          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.4985406532314107\n",
      "hamming loss: 0.16618021774380357\n",
      "\n",
      "iteration: 24\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "4          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "5  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.5045170257123002\n",
      "hamming loss: 0.16817234190410008\n",
      "\n",
      "iteration: 25\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.6661570535093815\n",
      "hamming loss: 0.22205235116979383\n",
      "\n",
      "iteration: 26\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "4          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "5  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.5036831132731063\n",
      "hamming loss: 0.16789437109103544\n",
      "\n",
      "iteration: 27\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "4    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "5  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "hamming distance: 0.5046560111188325\n",
      "hamming loss: 0.16821867037294416\n",
      "\n",
      "iteration: 28\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "4  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "5  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "hamming distance: 0.5045170257123002\n",
      "hamming loss: 0.16817234190410008\n",
      "\n",
      "iteration: 29\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.6674079221681724\n",
      "hamming loss: 0.22246930738939077\n",
      "\n",
      "iteration: 30\n",
      "the best K = 6\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "2  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "3  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "4          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "5    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.5046560111188325\n",
      "hamming loss: 0.16821867037294416\n",
      "\n",
      "iteration: 31\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.7009034051424601\n",
      "hamming loss: 0.23363446838082003\n",
      "\n",
      "iteration: 32\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.7160528144544823\n",
      "hamming loss: 0.23868427148482743\n",
      "\n",
      "iteration: 33\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.7175816539263378\n",
      "hamming loss: 0.23919388464211258\n",
      "\n",
      "iteration: 34\n",
      "the best K = 3\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.7154968728283531\n",
      "hamming loss: 0.238498957609451\n",
      "\n",
      "iteration: 35\n",
      "the best K = 7\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "4  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "5  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "6    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.5595552466990966\n",
      "hamming loss: 0.18651841556636553\n",
      "\n",
      "iteration: 36\n",
      "the best K = 3\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.71591382904795\n",
      "hamming loss: 0.23863794301598332\n",
      "\n",
      "iteration: 37\n",
      "the best K = 5\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "4    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.59819318971508\n",
      "hamming loss: 0.19939772990502663\n",
      "\n",
      "iteration: 38\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.8401667824878388\n",
      "hamming loss: 0.28005559416261294\n",
      "\n",
      "iteration: 39\n",
      "the best K = 5\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "4  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "hamming distance: 0.7043780403057679\n",
      "hamming loss: 0.23479268010192264\n",
      "\n",
      "iteration: 40\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.7357887421820709\n",
      "hamming loss: 0.24526291406069028\n",
      "\n",
      "iteration: 41\n",
      "the best K = 5\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "2  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "4          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.5032661570535094\n",
      "hamming loss: 0.16775538568450313\n",
      "\n",
      "iteration: 42\n",
      "the best K = 3\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.8472550382209868\n",
      "hamming loss: 0.2824183460736623\n",
      "\n",
      "iteration: 43\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.7160528144544823\n",
      "hamming loss: 0.23868427148482743\n",
      "\n",
      "iteration: 44\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.66726893676164\n",
      "hamming loss: 0.2224229789205467\n",
      "\n",
      "iteration: 45\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.7002084781097985\n",
      "hamming loss: 0.23340282603659948\n",
      "\n",
      "iteration: 46\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "2          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "3          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "hamming distance: 0.7009034051424601\n",
      "hamming loss: 0.23363446838082003\n",
      "\n",
      "iteration: 47\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "2          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "3  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "hamming distance: 0.7021542738012508\n",
      "hamming loss: 0.23405142460041695\n",
      "\n",
      "iteration: 48\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "3    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
      "hamming distance: 0.7160528144544823\n",
      "hamming loss: 0.23868427148482743\n",
      "\n",
      "iteration: 49\n",
      "the best K = 7\n",
      "            Family          Genus                 Species\n",
      "0  Leptodactylidae      Adenomera  AdenomeraHylaedactylus\n",
      "1          Hylidae      Hypsiboas       HypsiboasCordobae\n",
      "2  Leptodactylidae      Adenomera  AdenomeraHylaedactylus\n",
      "3          Hylidae      Hypsiboas    HypsiboasCinerascens\n",
      "4  Leptodactylidae      Adenomera          AdenomeraAndre\n",
      "5  Leptodactylidae      Adenomera          AdenomeraAndre\n",
      "6          Hylidae  Leptodactylus     LeptodactylusFuscus\n",
      "hamming distance: 0.5673384294649062\n",
      "hamming loss: 0.1891128098216354\n",
      "\n",
      "iteration: 50\n",
      "the best K = 4\n",
      "            Family      Genus                 Species\n",
      "0  Leptodactylidae  Adenomera          AdenomeraAndre\n",
      "1  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
      "2          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
      "3          Hylidae  Hypsiboas       HypsiboasCordobae\n",
      "hamming distance: 0.7000694927032661\n",
      "hamming loss: 0.2333564975677554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hamming_dist = []\n",
    "hamming_loss = []\n",
    "for i in range(50):\n",
    "    print(f'iteration: {i+1}' )\n",
    "    best_k = get_best_k(range(2,51), df.iloc[:, :-4])\n",
    "    kmeans = KMeans(n_clusters=best_k)\n",
    "    clusters = kmeans.fit_predict(df.iloc[:, :-4])\n",
    "    cluster_major = majority_label(best_k, clusters, df.iloc[:, -4:-1])\n",
    "    print(cluster_major)\n",
    "    dist, loss = calculate_hamming_loss_dist(cluster_major, clusters, df.iloc[:, -4:-1])\n",
    "    hamming_dist.append(dist)\n",
    "    hamming_loss.append(loss)\n",
    "\n",
    "    print(f'hamming distance: {dist}\\nhamming loss: {loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average of hamming distance</th>\n",
       "      <th>std devation of hamming distance</th>\n",
       "      <th>average of hamming loss</th>\n",
       "      <th>std devation of hamming loss</th>\n",
       "      <th>average of hamming score</th>\n",
       "      <th>std devation of hamming score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7353</td>\n",
       "      <td>0.102765</td>\n",
       "      <td>0.2451</td>\n",
       "      <td>0.034255</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>0.034255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average of hamming distance  std devation of hamming distance  \\\n",
       "0                       0.7353                          0.102765   \n",
       "\n",
       "   average of hamming loss  std devation of hamming loss  \\\n",
       "0                   0.2451                      0.034255   \n",
       "\n",
       "   average of hamming score  std devation of hamming score  \n",
       "0                    0.7549                       0.034255  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"average of hamming distance\": [np.mean(hamming_dist)],\n",
    "        \"std devation of hamming distance\": [np.std(hamming_dist)],\n",
    "        \"average of hamming loss\": [np.mean(hamming_loss)],\n",
    "        \"std devation of hamming loss\": [np.std(hamming_loss)],\n",
    "        \"average of hamming score\": [1 - np.mean(hamming_loss)],\n",
    "        \"std devation of hamming score\": [np.std(hamming_loss)]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISLR 12.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dissimilarity_matrix = np.array([\n",
    "    [0.0, 0.3, 0.4, 0.7],\n",
    "    [0.3, 0.0, 0.5, 0.8],\n",
    "    [0.4, 0.5, 0.0, 0.45],\n",
    "    [0.7, 0.8, 0.45, 0.0]\n",
    "])\n",
    "dist = squareform(dissimilarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3  0.4  0.7  0.5  0.8  0.45]\n"
     ]
    }
   ],
   "source": [
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = linkage(dist, \"complete\")\n",
    "dendrogram(z, labels=[\"0\", \"1\", \"2\", \"3\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = linkage(dist, \"single\")\n",
    "dendrogram(z, labels=[\"0\", \"1\", \"2\", \"3\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{0, 1}  \n",
    "{2, 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{0, 1, 2}  \n",
    "{3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dendrogram(\n",
    "    [[2.  , 3.  , 0.45, 2.  ],\n",
    "    [0.  , 1.  , 0.3 , 2.  ],\n",
    "    [4.  , 5.  , 0.8 , 4.  ]]\n",
    "    ,labels=[\"0\", \"1\", \"2\", \"3\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSCI 552",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
